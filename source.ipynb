{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "import requests\n",
    "import speech_recognition as sr\n",
    "import webbrowser\n",
    "import tkinter as tk\n",
    "import cv2\n",
    "import threading\n",
    "import face_recognition\n",
    "import pywhatkit\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import torch \n",
    "#from supervision.detection.core import Detections\n",
    "from supervision.tools.detections import Detections, BoxAnnotator\n",
    "from supervision.draw.color import ColorPalette\n",
    "from PIL import Image, ImageTk\n",
    "import dlib\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import imageio\n",
    "from tkinter import PhotoImage\n",
    "\n",
    "\n",
    "class FaceRecognitionApp():\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Voice Assistant with Face Recognition\")\n",
    "        self.root.geometry(\"1280x720\")  \n",
    "        self.robot_label = tk.Label(root, text=\"ðŸ¤–\", font=(\"Arial\", 100),fg=\"black\")\n",
    "        self.robot_label.pack(pady=20)\n",
    "        self.capture = None\n",
    "        self.r = sr.Recognizer()\n",
    "        self.recognized_gender=\"female\"\n",
    "        self.output_text = tk.Text(self.root, height=6, width=40)\n",
    "        self.output_text.pack(pady=10)\n",
    "        #pil_image = Image.open(bg_image_path)\n",
    "        #tk_image = ImageTk.PhotoImage(pil_image)\n",
    "\n",
    "        # Store tk_image as an attribute\n",
    "\n",
    "        # Create a canvas with the background image\n",
    "        self.verification_textbox = tk.Text(self.root, height=2, width=40)\n",
    "        self.verification_textbox.pack(pady=10)\n",
    "    \n",
    "        # Create a Label for displaying the GIF\n",
    "        self.verification_label = tk.Label(self.root)\n",
    "        self.verification_label.pack(pady=10)\n",
    "        \n",
    "        # Load the animated GIF frames using Pillow\n",
    "        self.verification_gif_path = r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\loading-icon-animated-gif-19-1.gif\"\n",
    "\n",
    "        self.assistant_status_textbox = tk.Text(self.root, height=2, width=40)\n",
    "        self.assistant_status_textbox.pack(pady=10)\n",
    "\n",
    "        self.root.bind(\"<<UpdateUI2>>\", lambda event=None: self.update_ui_2())\n",
    "        \n",
    "        # a label for the back camera\n",
    "        self.camera_label = tk.Label(root)\n",
    "        self.camera_label.pack()\n",
    "        self.stop_back_camera = threading.Event()\n",
    "        self.back_camera_thread = None \n",
    "        self.recognized=False\n",
    "        self.dms_stop_event = threading.Event()\n",
    "        self.dms_thread = threading.Thread(target=self.start_dms)\n",
    "\n",
    "        self.speaking_flag = False\n",
    "\n",
    "        self.alert_sound = AudioSegment.from_mp3(r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\alert_sound.wav\")\n",
    "        self.playback = None\n",
    "\n",
    "        # binding the window closing to a function that stops the dms camera\n",
    "        self.root.protocol(\"WM_DELETE_WINDOW\", self.on_close)\n",
    "        \n",
    "        # Start the face recognition and voice assistant\n",
    "        threading.Thread(target=self.start_face_recognition).start()\n",
    "\n",
    "\n",
    "\n",
    "    def show_camera_feed(self):\n",
    "        while True:\n",
    "            if self.capture is not None:\n",
    "                ret, frame = self.capture.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Resize the frame to fit the Tkinter window\n",
    "                frame = cv2.resize(frame, (1000, 800))\n",
    "        \n",
    "                # Convert the OpenCV frame to Tkinter PhotoImage\n",
    "                tk_image = self.convert_opencv_to_tk(frame)\n",
    "        \n",
    "                # Update the label with the new image\n",
    "                self.camera_label.config(image=tk_image)\n",
    "                self.camera_label.image = tk_image\n",
    "        \n",
    "                if self.recognized==True and self.capture is not None:\n",
    "                    self.camera_label.image = None\n",
    "                    break\n",
    "\n",
    "\n",
    "    def open_camera_and_predict_face(self):\n",
    "        self.verification_textbox.delete(1.0, tk.END)\n",
    "        self.verification_textbox.insert(tk.END, \"Verifying...\")\n",
    "        gif_viewer = AnimatedGIFViewer(self.root, self.verification_gif_path)\n",
    "        \n",
    "        # Load the image of the person to be recognized\n",
    "        known_image_path = r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\my_image_cropped.jpg\"\n",
    "        known_image = face_recognition.load_image_file(known_image_path)\n",
    "        known_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "        known_gender = 'male'\n",
    "        known_person = {'encoding': known_encoding, 'gender': known_gender}\n",
    "\n",
    "        self.capture = cv2.VideoCapture(0)\n",
    "        gif_viewer.stop()\n",
    "        threading.Thread(target=self.show_camera_feed).start()\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.capture.read()\n",
    "            # Find all face locations in the current frame\n",
    "            face_locations = face_recognition.face_locations(frame)\n",
    "            \n",
    "            if face_locations:\n",
    "                # Encode the first face found in the frame\n",
    "                current_encoding = face_recognition.face_encodings(frame, face_locations)[0]\n",
    "\n",
    "                # Compare the current face encoding with the known encoding\n",
    "                results = face_recognition.compare_faces([known_person['encoding']], current_encoding)\n",
    "        \n",
    "                if results[0]:\n",
    "                    self.recognized=True\n",
    "                    recognized_gender = known_person['gender']\n",
    "                    self.recognized_gender=recognized_gender\n",
    "                    self.verification_textbox.delete(1.0, tk.END)\n",
    "                    self.verification_textbox.insert(tk.END, f\"Verified - Gender: {recognized_gender}\")\n",
    "                    self.assistant_status_textbox.insert(tk.END, \"Assisting...\")\n",
    "                    break\n",
    "\n",
    "        threading.Thread(target=self.start_voice_assistant).start()\n",
    "        self.dms_thread.start()\n",
    "\n",
    "    def start_voice_assistant(self):\n",
    "        gif_viewer = None\n",
    "        \n",
    "        with sr.Microphone() as mic:\n",
    "            while True:\n",
    "                if gif_viewer:\n",
    "                    gif_viewer.stop()\n",
    "                self.camera_label.config(image=None)\n",
    "                gif_viewer = AnimatedGIFViewer(self.root, r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\bddf8a11582713.560fa0db0dee5.gif\")\n",
    "                try:\n",
    "                    self.root.event_generate(\"<<UpdateUI2>>\")\n",
    "                    self.speak(\"jarvis is listening for the wake word\",self.recognized_gender)\n",
    "                    self.r.adjust_for_ambient_noise(mic, duration=0.2)\n",
    "                    audio = self.r.listen(mic,timeout=4,phrase_time_limit=4)\n",
    "                    \n",
    "                    text = self.r.recognize_google(audio)\n",
    "                    text = text.lower()\n",
    "                    if \"hey jarvis\" in text:\n",
    "                        gif_viewer.stop()\n",
    "                        self.root.after(100, self.update_ui_color)\n",
    "                        self.speak(\"what can i do for you today sir\",self.recognized_gender)\n",
    "                        audio = self.r.listen(mic,timeout=4,phrase_time_limit=4)\n",
    "                        text = self.r.recognize_google(audio)\n",
    "                        text = text.lower()\n",
    "                        \n",
    "                        if text is not None:\n",
    "                            self.root.after(100, self.update_ui_1,text)\n",
    "                            if \"how are you\" in text:\n",
    "                                self.speak(\"Never been better, master wahdan\",self.recognized_gender)\n",
    "                            elif \"hello\" in text:\n",
    "                                self.speak(\"Hi there.\",self.recognized_gender)\n",
    "                            elif \"open google\" in text:\n",
    "                                webbrowser.open_new('http://google.com')\n",
    "                            elif \"go to sleep\" in text:\n",
    "                                self.speak(\"Okay, goodnight sir.\",self.recognized_gender)\n",
    "                                self.on_close()\n",
    "                                exit()\n",
    "                            \n",
    "                            elif \"google\" in text:\n",
    "                                google_index = text.index(\"google\")\n",
    "                                search_query = text[google_index + len(\"google\"):].strip()\n",
    "                                pywhatkit.search(search_query)\n",
    "                                \n",
    "                            elif \"youtube\" in text:\n",
    "                                google_index = text.index(\"youtube\")\n",
    "                                search_query = text[google_index + len(\"youtube\"):].strip()\n",
    "                                pywhatkit.playonyt(search_query)\n",
    "\n",
    "                            elif \"spotify\" in text:\n",
    "                                spotify_index = text.index(\"spotify\")\n",
    "                                search_query = text[spotify_index + len(\"spotify\"):].strip()\n",
    "                                self.play_song_on_spotify_app(search_query)\n",
    "                            \n",
    "                            elif \"camera\" in text and \"stop\" in text:\n",
    "                                self.close_back_camera()\n",
    "                                self.speak(\"Stopping back camera\",self.recognized_gender)\n",
    "                            \n",
    "                            elif \"camera\" in text and \"open\" in text:\n",
    "                                if not self.back_camera_thread or not self.back_camera_thread.is_alive():\n",
    "                                    self.back_camera_thread = threading.Thread(target=self.open_back_camera)\n",
    "                                    self.back_camera_thread.start()\n",
    "                                else:\n",
    "                                    self.speak(\"The back camera is already active.\",self.recognized_gender)\n",
    "                                \n",
    "                            elif \"weather\" in text:\n",
    "                                self.get_and_speak_weather()\n",
    "                            \n",
    "                            else:\n",
    "                                self.speak(\"I'm sorry, sir. I did not understand your request.\",self.recognized_gender)\n",
    "\n",
    "                except sr.UnknownValueError:\n",
    "                    continue\n",
    "                except sr.RequestError:\n",
    "                    continue\n",
    "                except sr.exceptions.WaitTimeoutError:\n",
    "                    continue\n",
    "\n",
    "    def start_face_recognition(self):\n",
    "        # Start face recognition before voice  assistant\n",
    "        self.open_camera_and_predict_face()\n",
    "\n",
    "        \n",
    "    def update_ui_1(self,text):\n",
    "        # This function updates the UI from the main thread\n",
    "        self.output_text.insert(tk.END, text+\"\\n\")\n",
    "        \n",
    "    def update_ui_color(self):\n",
    "        # This function updates the UI from the main thread\n",
    "        self.robot_label.config(fg=\"red\")\n",
    "    \n",
    "    def update_ui_2(self):\n",
    "        # This function updates the UI from the main thread\n",
    "        self.robot_label.config(fg=\"black\")\n",
    "        \n",
    "    \n",
    "    def speak(self, text, gender):\n",
    "        if self.speaking_flag:\n",
    "            return       \n",
    "\n",
    "        self.speaking_flag = True\n",
    "\n",
    "        rate = 100\n",
    "        engine = pyttsx3.init()\n",
    "        voices = engine.getProperty('voices')\n",
    "\n",
    "        # Set voice based on gender\n",
    "        if gender.lower() == 'male':\n",
    "            engine.setProperty('voice', voices[0].id)  # Assuming the first voice is male\n",
    "        elif gender.lower() == 'female':\n",
    "            engine.setProperty('voice', voices[1].id)  # Assuming the second voice is female\n",
    "\n",
    "        engine.setProperty('rate', rate + 75)\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "        self.speaking_flag = False\n",
    "\n",
    "    def play_song_on_spotify_app(self, song_name):\n",
    "            search_url = f\"spotify:search:{song_name}\"\n",
    "            os.system(f\"start {search_url}\")\n",
    "    \n",
    "    def load_yolo(self, model_path): \n",
    "        model = YOLO(model_path)\n",
    "        model.fuse()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    def open_back_camera(self):\n",
    "        model = self.load_yolo(r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\CX_Deployment\\yolov8s.pt\")\n",
    "\n",
    "        if self.capture is None:\n",
    "            self.capture = cv2.VideoCapture(0)\n",
    "        \n",
    "        # dict maping class_id to class_name\n",
    "        CLASS_NAMES_DICT = model.model.names\n",
    "        box_annotator = BoxAnnotator(color=ColorPalette(), thickness=1, text_thickness=1, text_scale=0.5)\n",
    "\n",
    "        # Use if the voice command didn't work\n",
    "        self.root.bind_all(\"<KeyPress-q>\", self.close_back_camera)\n",
    "        \n",
    "        while not self.stop_back_camera.is_set() and self.back_camera_thread.is_alive():\n",
    "            # Read a frame from the camera\n",
    "            ret, frame = self.capture.read()\n",
    "        \n",
    "            if not ret or self.stop_back_camera.wait(timeout=0.07):\n",
    "                break\n",
    "            \n",
    "            # Perform inference on the GPU\n",
    "            results = model(frame)\n",
    "            detections = Detections(\n",
    "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
    "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
    "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "            )\n",
    "            \n",
    "            # format custom labels\n",
    "            labels = [\n",
    "                f\"{CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\"\n",
    "                for _, confidence, class_id, tracker_id\n",
    "                in detections\n",
    "            ]\n",
    "        \n",
    "            # annotate and display frame\n",
    "            frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n",
    "        \n",
    "            # Resize the frame to fit the Tkinter window\n",
    "            frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "            # Convert the OpenCV frame to Tkinter PhotoImage\n",
    "            tk_image = self.convert_opencv_to_tk(frame)\n",
    "\n",
    "            # Update the label with the new image\n",
    "            self.camera_label.config(image=tk_image)\n",
    "            self.camera_label.image = tk_image\n",
    "        \n",
    "            self.root.update_idletasks()\n",
    "\n",
    "        \n",
    "    def convert_opencv_to_tk(self, frame):\n",
    "            \"\"\"Convert OpenCV image to Tkinter PhotoImage.\"\"\"\n",
    "            b, g, r = cv2.split(frame)\n",
    "            img = cv2.merge((r, g, b))\n",
    "            img = Image.fromarray(img)\n",
    "            tk_image = ImageTk.PhotoImage(image=img)\n",
    "            return tk_image\n",
    "\n",
    "    def close_back_camera(self, event=None):\n",
    "        if self.back_camera_thread and self.back_camera_thread.is_alive():\n",
    "            # Set the stop_back_camera flag to signal thread termination\n",
    "            self.stop_back_camera.set()\n",
    "            # Wait for the thread to finish before continuing\n",
    "            self.back_camera_thread.join()\n",
    "            # Reset the flag for potential future use\n",
    "            self.stop_back_camera.clear()\n",
    "            # Schedule the cleanup actions in the main thread using after\n",
    "            self.root.after(10, self.cleanup_back_camera)\n",
    "\n",
    "    def cleanup_back_camera(self):\n",
    "        self.camera_label.config(image=None)\n",
    "        self.camera_label.image = None\n",
    "        \n",
    "\n",
    "    def start_dms(self):\n",
    "        landmarks_window, landmarks_label = self.create_landmarks_window()\n",
    "        \n",
    "        # Load the facial landmark predictor\n",
    "        predictor = dlib.shape_predictor(r\"C:\\Users\\ahmed\\Desktop\\JARVIS_GITHUB\\JARVIS_V0.1\\CX_Deployment\\shape_predictor_68_face_landmarks.dat\\shape_predictor_68_face_landmarks.dat\")\n",
    "        face_detector = dlib.get_frontal_face_detector()\n",
    "        \n",
    "        # Threshold for blink detection\n",
    "        EAR_THRESHOLD = 0.25\n",
    "\n",
    "        self.capture = cv2.VideoCapture(0)\n",
    "        \n",
    "        # Blink timer variables\n",
    "        blink_start_time = 0\n",
    "        blink_duration = 0\n",
    "        blink_duration_first_threshold = 3\n",
    "        blink_duration_second_threshold = 5\n",
    "        first_alert_given = False\n",
    "        \n",
    "        while not self.dms_stop_event.is_set() and self.dms_thread.is_alive():\n",
    "            ret, frame = self.capture.read()\n",
    "\n",
    "            if not ret or self.dms_stop_event.is_set():\n",
    "                break\n",
    "            \n",
    "            # Convert the frame to grayscale\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "            # Use dlib to detect faces\n",
    "            faces = face_detector(gray)\n",
    "            \n",
    "            for face in faces:\n",
    "                # Get facial landmarks\n",
    "                landmarks = predictor(gray, face)\n",
    "                landmarks = np.array([(landmark.x, landmark.y) for landmark in landmarks.parts()])\n",
    "\n",
    "                for (x, y) in landmarks:\n",
    "                    cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "    \n",
    "                # Update the landmarks window\n",
    "                landmarks_window.update_idletasks()\n",
    "                \n",
    "                # Extract left and right eye coordinates\n",
    "                left_eye = landmarks[42:48]\n",
    "                right_eye = landmarks[36:42]\n",
    "        \n",
    "                # Compute eye aspect ratios\n",
    "                left_ear = self.eye_aspect_ratio(left_eye)\n",
    "                right_ear = self.eye_aspect_ratio(right_eye)\n",
    "                    \n",
    "                # Check for blinks\n",
    "                if left_ear < EAR_THRESHOLD and right_ear < EAR_THRESHOLD:\n",
    "                    if blink_start_time == 0:\n",
    "                        blink_start_time = time.time()\n",
    "                    blink_duration = time.time() - blink_start_time\n",
    "                else:\n",
    "                    blink_start_time = 0\n",
    "                    blink_duration = 0\n",
    "                    first_alert_given = False\n",
    "        \n",
    "                if blink_duration >= blink_duration_first_threshold and not first_alert_given:\n",
    "                    self.speak(\"You may be tired, Consider taking a break\", self.recognized_gender)\n",
    "                    first_alert_given = True\n",
    "        \n",
    "                if blink_duration >= blink_duration_second_threshold:\n",
    "                    play(self.alert_sound)\n",
    "\n",
    "            resized_frame = cv2.resize(frame, (600, 600))\n",
    "            \n",
    "            # Convert the frame to Tkinter PhotoImage\n",
    "            tk_image = self.convert_opencv_to_tk(resized_frame)\n",
    "\n",
    "            # Update the label with the new image\n",
    "            landmarks_label.config(image=tk_image)\n",
    "            landmarks_label.image = tk_image\n",
    "        \n",
    "        if not self.dms_thread.is_alive():\n",
    "            # Release the webcam and close the window\n",
    "            landmarks_window.destroy()\n",
    "            self.capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "    def get_and_speak_weather(self):\n",
    "        # Function to get weather information and speak it to the user\n",
    "        api_key = '6ba0421f5c194ad2a15183722241003'  # Replace with your actual WeatherAPI.com API key\n",
    "        city_name = 'Cairo'  # Replace with the desired city name or extract it from user input\n",
    "\n",
    "        base_url = \"https://api.weatherapi.com/v1/current.json\"\n",
    "        params = {\n",
    "            'key': api_key,\n",
    "            'q': city_name,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            if 'error' in data:\n",
    "                print(f\"Error: {data['error']['message']}\")\n",
    "                return\n",
    "\n",
    "            temperature = data['current']['temp_c']\n",
    "            condition = data['current']['condition']['text']\n",
    "\n",
    "            # Speak the weather details\n",
    "            weather_message = f\"The current temperature in {city_name} is {temperature}Â°C, and the weather condition is {condition}.\"\n",
    "            self.speak(weather_message, self.recognized_gender)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def eye_aspect_ratio(self, eye):\n",
    "        # Calculate Euclidean distances between the two sets of vertical landmarks (y-coordinates)\n",
    "        a = np.linalg.norm(eye[1] - eye[5])\n",
    "        b = np.linalg.norm(eye[2] - eye[4])\n",
    "    \n",
    "        # Calculate Euclidean distance between the horizontal landmarks (x-coordinates)\n",
    "        c = np.linalg.norm(eye[0] - eye[3])\n",
    "    \n",
    "        # Compute the eye aspect ratio\n",
    "        ear = (a + b) / (2.0 * c)\n",
    "        return ear\n",
    "\n",
    "    def on_close(self):\n",
    "        if self.dms_thread.is_alive():\n",
    "            # Set the flag to signal the DMS thread to exit\n",
    "            self.dms_stop_event.set()\n",
    "            \n",
    "            # Stop the DMS thread if it's running\n",
    "            self.dms_thread.join()\n",
    "\n",
    "        # Release the camera resources\n",
    "        if self.capture:\n",
    "            self.capture.release()\n",
    "\n",
    "        # Close the main window\n",
    "        self.root.destroy()\n",
    "\n",
    "    def create_landmarks_window(self):\n",
    "        landmarks_window = tk.Toplevel(self.root)\n",
    "        landmarks_window.title(\"Facial Landmarks\")\n",
    "        landmarks_window.geometry(\"600x600\")\n",
    "        landmarks_label = tk.Label(landmarks_window)\n",
    "        landmarks_label.pack()\n",
    "        return landmarks_window, landmarks_label\n",
    "        \n",
    "class AnimatedGIFViewer:\n",
    "    def __init__(self, master, gif_path):\n",
    "        self.master = master\n",
    "        self.frames = self.load_frames(gif_path)\n",
    "        self.current_frame = 0\n",
    "        self.image_label = tk.Label(self.master)\n",
    "        self.image_label.pack()\n",
    "        self.running = True\n",
    "        self.display_frame()\n",
    "\n",
    "    def load_frames(self, gif_path):\n",
    "        gif = Image.open(gif_path)\n",
    "        frames = []\n",
    "        try:\n",
    "            while True:\n",
    "                frame = ImageTk.PhotoImage(gif.copy())\n",
    "                frames.append(frame)\n",
    "                gif.seek(len(frames))  # Move to the next frame\n",
    "        except EOFError:\n",
    "            return frames  # End of frames\n",
    "    \n",
    "    def display_frame(self):\n",
    "        if not self.running:\n",
    "            return\n",
    "            \n",
    "        self.current_frame = (self.current_frame + 1) % len(self.frames)\n",
    "        self.image_label.config(image=self.frames[self.current_frame])\n",
    "        self.image_label.image = self.frames[self.current_frame]\n",
    "        self.master.after(50, self.display_frame)  # Adjust the interval for frame update\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.image_label.destroy()  # Remove the label from the UI\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = tk.Tk()\n",
    "    app = FaceRecognitionApp(root)\n",
    "    root.mainloop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CX_Deploy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
